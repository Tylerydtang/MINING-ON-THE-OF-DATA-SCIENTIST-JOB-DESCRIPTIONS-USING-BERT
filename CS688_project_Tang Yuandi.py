# -*- coding: utf-8 -*-
"""699 project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zMOnQoImlaAdMHf-MLP4LFbzVIPrVwyN
"""

##699 A2 Fall 2022 Term Project
##Author: Yuandi Tang
##ID: U65674688
##Topic: Text Analysis with Bert on Data Scientist Job Descriptions
##Research Objectives: Keyword Extraction, Similarity compare, Neuron Network Map, Q&A


#PREPROCESSING#
#import libraries
!pip install nltk
!pip install keybert
!pip install transformers

import string 
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from torch.nn import functional as F
from smart_open.smart_open_lib import doctools
from locale import delocalize
from keybert import KeyBERT
from collections import Counter
from gensim.summarization import summarize
from pyparsing.helpers import WordStart
from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertForNextSentencePrediction, AutoModelForQuestionAnswering
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import torch


#PREPROCESSING
doc1 = open("DS-JD1.txt") #import fist job description
doc2 = open("DS-JD2.txt") #import 2nd job description
doc3 = open("DS-JD3.txt") #import 3rd job description
doc4 = open("DS-JD4.txt") #import 4th job description
doc5 = open("DS-JD5.txt") #import 5th job description
doc6 = open("DS-JD6.txt") #import 6th job description
doc7 = open("DS-JD7.txt") #import 7th job description
doc8 = open("DS-JD8.txt") #import 8th job description
doc9= open("DS-JD9.txt") #import 9th job description
doc10= open("DS-JD10.txt") #import 10th job description
doc11 = open("DS-JD11.txt") #import 11th job description
doc12 = open("DS-JD12.txt") #import 12th job description
doc13 = open("DS-JD13.txt") #import 13th job description
doc14 = open("DS-JD14.txt") #import 14th job description
doc15 = open("DS-JD15.txt") #import 15th job description
doc16 = open("DS-JD16.txt") #import 16th job description
doc17 = open("DS-JD17.txt") #import 17th job description
doc18 = open("DS-JD18.txt") #import 18th job description
doc19= open("DS-JD19.txt") #import 19th job description
doc20= open("DS-JD20.txt") #import 20th job description

#bind them together to a text string
doc= doc1.read()+doc2.read()+doc3.read()+doc4.read()+doc5.read()+doc6.read()+doc7.read()+doc8.read()+doc9.read()+doc10.read()+doc11.read()+doc12.read()+doc13.read()+doc14.read()+doc15.read()+doc16.read()+doc17.read()+doc18.read()+doc19.read()+doc20.read()

#tokenize words
docx = word_tokenize(doc)

# convert to lower case
docx = [w.lower() for w in docx]

# remove punctuation from each word
table = str.maketrans('', '', string.punctuation)
stripped = [w.translate(table) for w in docx]

# remove remaining tokens that are not alphabetic
docx = [word for word in stripped if word.isalpha()]

# filter out some words
stop_words = set(stopwords.words('english'))  #get stop words
job_names = ("data","scientist") #get job name words
docx = [w for w in docx if not w in stop_words]  #remove stop words
docx = [w for w in docx if not w in job_names]  #remove job name words
print(docx) #check tokenized doc


##ALGORITHM VALIDATION##
#SIMILARITY
# Semantic Similarity on Scientific Text
# Define Model & Tokenizer
# model_name = "distilroberta-base"  # Distilroberta Model
model_name = "bert-base-uncased"  # Plain Bert Model
# model_name = "GanjinZero/UMLSBert_ENG"  # CODER UMLS Medical Bert Model
model = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
S3 = sent_tokenize(doc)[2]  #3rd sentence in JD
S4 = sent_tokenize(doc)[3] #4th sentence in JD
S100 =sent_tokenize(doc)[99]  #100th sentence in JD
# query = "the MRI of the abdomen is normal and without evidence of malignancy."
# sent_1 = "no significant abnormalities involving the abdomen is observed."
# sent_2 = "deformity of the ventral thecal sac is observed."
# Tokenize Sentences
inputs_0 = tokenizer(S3, return_tensors='pt')   # Get tokens as tensors
inputs_1 = tokenizer(S4, return_tensors='pt')   # Get tokens as tensors
inputs_2 = tokenizer(S100, return_tensors='pt')   # Get tokens as tensors
#  Get the Embeddings
sent_0_embed = np.mean(model(**inputs_0).last_hidden_state[0].detach().numpy(), axis=0, keepdims=True)
sent_1_embed = np.mean(model(**inputs_1).last_hidden_state[0].detach().numpy(), axis=0, keepdims=True)
sent_2_embed = np.mean(model(**inputs_2).last_hidden_state[0].detach().numpy(), axis=0, keepdims=True)
# Calculate Cosine Symilarity between the 2 texts
similarities_q1 = cosine_similarity(sent_0_embed, sent_1_embed)  # Find Cosine Symilarity between the 2 texts
similarities_q2 = cosine_similarity(sent_0_embed, sent_2_embed)  # Find Cosine Symilarity between the 2 texts
print("Query Symilarity with Sentence 1 ---", similarities_q1[0][0])
print("Query Symilarity with Sentence 2 ---", similarities_q2[0][0])
print("==============")
print("End")
#sentences nearby have logical connections, thus has more similarities than random sentences pairs. 


##NEXT SENTENCE PREDICTION##
#Validation of the Bert algorithm's ability to recognise sentences
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')
first_sentence = sent_tokenize(doc)[2]  #34d sentence in jd1 
next_sentence1 = sent_tokenize(doc)[3] #4th sentence in jd
next_sentence2 = sent_tokenize(doc)[103] #14th sentence in jd
print("~~~~~~~~~~~~~~~~~~~~~~")
print(first_sentence)
print("~~~~~~~~~~~~~~~~~~~~~~")
for next_sentence in [next_sentence1, next_sentence2]:
    encoding = tokenizer.encode_plus(first_sentence, next_sentence, return_tensors='pt')
    outputs = model(**encoding)[0]
    softmax = F.softmax(outputs, dim=1)

    if softmax[0][0] > softmax[0][1]:
        print(next_sentence)
        print("This sentence is the better following sentence")
        print(softmax)
        print("==== ==== ==== ")
    else:
        print(next_sentence)
        print("This sentence is not the better following sentence")
        print(softmax)
        print("==== ==== ==== ")
print("End")
#bert successfully chose the following sentences.

#TEXT MINING#
#top frequency words
Counter = Counter(docx) #input doc to counter
freq = Counter.most_common(75) #show most frequent 75 words
#freq = list(freq)
print(freq) #print frequency in list
#output barchart
list=[] #void list
i=0
c=range(0,30)
for i in c:
  if i <30:
    list += freq[i]
    i=i+1
else:
    mostword = list[0:60:2] #get words
    frequency= list[1:60:2] #get frequencies
#plot frequencies in barplot
plt.bar(mostword, frequency)
plt.xticks(rotation=90) 
plt.xlabel('Words')
plt.ylabel('Frequencies')
plt.title('Top 30 Frequent words')
plt.show() 


#KEYWORD EXTRACTION#
doc = doc
model = KeyBERT('distilbert-base-nli-mean-tokens')
keywords = model.extract_keywords(doc, top_n=5) #single word keywords
keywords1 = model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5) #Max Sum Similarity
keywords2 = model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_mmr=True, diversity=0.7, top_n=5) # Maximal Marginal Relevance
# Sentence-Transformers Modles
# You can select any model from sentence-transformers here and pass it through KeyBERT with model:
model = KeyBERT(model='distilbert-base-nli-mean-tokens')
keywords3 = model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english', use_maxsum=True, nr_candidates=20, top_n=5)
print(keywords)
print(keywords1)
print(keywords2)
print(keywords3)


#TEXT SUMMARIZATION#
summary_Gensim = summarize(doc, ratio=0.01)  # Summarize using Gensim for the entire Document
print(summary_Gensim) #print Gensim


#Q&A#
tokenizer = AutoTokenizer.from_pretrained("deepset/bert-base-cased-squad2") #tokenize models
model = AutoModelForQuestionAnswering.from_pretrained("deepset/bert-base-cased-squad2")
question = "what is data scientist?"
text = summary_Gensim #load summary text
tokenizer = AutoTokenizer.from_pretrained("deepset/bert-base-cased-squad2") #tokenize models
model = AutoModelForQuestionAnswering.from_pretrained("deepset/bert-base-cased-squad2")
inputs = tokenizer.encode_plus(question, text, return_tensors="pt")  # Take the question & text as text and returns tokens (model-ingestible format).
answer_start_scores, answer_end_scores = model(**inputs, return_dict=False) #Calculate Model Score using AutoModelForQuestionAnswering
answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score
answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score
the_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs["input_ids"][0][answer_start:answer_end])) #get the answers
print('==================')
print('Question: ' + question)
print('Answer: ' + the_answer)
print('==================')
print('End') #print the answers
#Data scientists are engineers, that's true!